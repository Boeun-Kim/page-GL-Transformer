---
layout: default
---

<header>
{% if site.theme_config.show_navbar == true %}
  {% include horizontal_list.html collection=site.data.home.navbar_entries %}
  <div class="dashed"></div>
{% endif %}

  <h1>{{ site.title }}</h1>
  {% if site.theme_config.show_description == true %}
    <p>{{ site.description }}</p>
  {% endif %}
</header>
<body>
  <div style="text-align:center">
  <iframe width="650" height="366" src="https://www.youtube.com/embed/OwCJlmWxS9A" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  <div style="text-align:left">
  <h2>Abstract</h2>
  <p>We propose a new transformer model for the task of unsupervised learning of skeleton motion sequences. The existing transformer model utilized for unsupervised skeleton-based action learning is learned the instantaneous velocity of each joint from adjacent frames without global motion information. Thus, the model has difficulties in learning the attention globally over whole-body motions and temporally distant joints. In addition, person-to-person interactions have not been considered in the model. To tackle the learning of whole-body motion, long-range temporal dynamics, and person-to-person interactions, we design a global and local attention mechanism, where, global body motions and local joint motions pay attention to each other. In addition, we propose a novel pretraining strategy, multi-interval pose displacement prediction, to learn both global and local attention in diverse time ranges. The proposed model successfully learns local dynamics of the joints and captures global context from the motion sequences. Our model outperforms state-of-the-art models by notable margins in the representative benchmarks.</p>
  <h2>Citation</h2>
  
  <pre>
  @inproceedings{kim2022global,
  title={Global-local motion transformer for unsupervised skeleton-based action learning},
  author={Kim, Boeun and Chang, Hyung Jin and Kim, Jungho and Choi, Jin Young},
  booktitle={European conference on computer vision},
  pages={209--225},
  year={2022},
  organization={Springer}
  }
  </pre>
    
</body>



{{ content }}

{% if site.theme_config.show_projects == true %}
  <h2>{{ site.theme_config.home.title_projects }}</h2>
  {% include card_list.html collection=site.data.home.project_entries %}
{% endif %}

{% if site.theme_config.show_misc_list == true %}
  <h2>{{ site.theme_config.home.title_misc_list }}</h2>
  {% include vertical_list.html collection=site.data.home.misc_entries %}
{% endif %}

{% if site.theme_config.show_blog == true %}
  <h2>{{ site.theme_config.home.title_blog }}</h2>
  {% include post_list.html %}
{% endif %}

{% if site.theme_config.show_old_projects == true %}
  <h2>{{ site.theme_config.home.title_old_projects }}</h2>
  {% include card_list.html collection=site.data.home.old_project_entries %}
{% endif %}


{% if site.theme_config.show_footer == true %}
  <footer>
    <div class="dashed"></div>
    {% include horizontal_list.html collection=site.data.home.footer_entries %}
  </footer>
{% endif %}
